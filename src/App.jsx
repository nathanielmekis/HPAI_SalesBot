import React, { useEffect, useRef, useState } from "react";
import { motion } from "framer-motion";
import { Mic, Square, Volume2, Loader2, ArrowUp } from "lucide-react";

// Toby Clone Bot â€“ Helport AI
// Apple-inspired voice chat UI with proper logo reference
// Use Vite env to override in dev if you don't set a proxy:
const WS_URL = import.meta.env.VITE_WS_URL || "/api/voicechat";
const ACCENT = "#00C389";
const BG_GRADIENT = `radial-gradient(1200px 600px at 50% -200px, rgba(0,195,137,0.14), transparent),
                     radial-gradient(800px 400px at 90% -100px, rgba(99,102,241,0.10), transparent)`;

const ABBR = [
  "U.S.", "U.K.", "e.g.", "i.e.", "etc.", "vs.",
  "Mr.", "Mrs.", "Ms.", "Dr.", "Prof.",
  "Inc.", "Ltd.", "Jr.", "Sr.", "St.", "No.",
  "Jan.", "Feb.", "Mar.", "Apr.", "Jun.", "Jul.", "Aug.", "Sep.", "Sept.", "Oct.", "Nov.", "Dec.",
];

function protectAbbrDots(s) {
  let out = s;
  for (const a of ABBR) {
    const safe = a.replace(/\./g, "Â§"); // ç”¨ Â§ æš‚ä»£å¥ç‚¹
    // ç²¾ç¡®æ›¿æ¢å¤§å°å†™åŒ¹é…ï¼ˆç®€å•èµ·è§ç”¨åŸæ ·å¤§å°å†™ï¼‰
    out = out.replaceAll(a, safe);
  }
  return out;
}
function restoreAbbrDots(s) { return s.replace(/Â§/g, "."); }

// æ™ºèƒ½åˆ†å¥ï¼šåœ¨ . ! ? åã€ä¸”åé¢åƒæ˜¯å¥é¦–ï¼ˆå¼•å·/æ‹¬å·/å¤§å†™/æ•°å­—ï¼‰å†æ–­
function splitIntoSentencesSmart(text) {
  if (!text) return [];
  const protectedText = protectAbbrDots(text);
  const parts = protectedText
    .replace(/\s+/g, " ")
    .trim()
    .split(/(?<=[.!?])\s+(?=["'(\[]?[A-Z0-9])/); // ç®€æ´æœ‰æ•ˆçš„å¯å‘å¼
  return parts.map(restoreAbbrDots).map(s => s.trim()).filter(Boolean);
}

// æŠŠå¥å­æŒ‰â€œ2â€“3å¥ / <= 400 å­—â€æ‰“åŒ…
function groupSentencesToChunks(sentences, { maxChars = 400, minSent = 2, maxSent = 3 } = {}) {
  const chunks = [];
  let cur = [];
  let curLen = 0;
  for (const s of sentences) {
    const willLen = curLen + (curLen ? 1 : 0) + s.length;
    if (cur.length >= maxSent || (cur.length >= minSent && willLen > maxChars)) {
      chunks.push(cur.join(" "));
      cur = [s];
      curLen = s.length;
    } else {
      cur.push(s);
      curLen = willLen;
    }
  }
  if (cur.length) chunks.push(cur.join(" "));
  return chunks;
}



// Pick a supported audio mime type (Safari prefers mp4/mpeg)
function pickAudioMime() {
  const candidates = [
    "audio/webm;codecs=opus",
    "audio/webm",
    "audio/mp4",    // Safari
    "audio/mpeg",   // Safari fallback
  ];
  for (const t of candidates) {
    try {
      if (window.MediaRecorder?.isTypeSupported?.(t)) return t;
    } catch {}
  }
  return ""; // let browser decide
}

export default function App() {
  const [mode, setMode] = useState("voice"); // 'voice' or 'type'
  const [textInput, setTextInput] = useState("");
  const [connected, setConnected] = useState(false);
  const [recording, setRecording] = useState(false);
  const [sessionId, setSessionId] = useState("");
  const [messages, setMessages] = useState([]);
  const [inputFocused, setInputFocused] = useState(false);
  const [conversationId, setConversationId] = useState(() => {
    try { return localStorage.getItem("dify_conversation_id") || ""; } catch { return ""; }
  });
  const [status, setStatus] = useState("Ready");

  const wsRef = useRef(null);
  const mediaRecorderRef = useRef(/** @type {MediaRecorder|null} */(null));
  const streamRef = useRef(/** @type {MediaStream|null} */(null));
  const audioRef = useRef(null);
  const scrollerRef = useRef(null);
  const API_BASE = import.meta.env.VITE_API_BASE || "";
  const AVATAR_URL = "/toby.png"; // lives in /public
  const START_FRESH_ON_LOAD = true;

  const chunksRef = useRef([]);
  const ASR_USER = "XBOT_DEV"; 

  const ttsCacheRef = useRef(new Map());

  // æ­£åœ¨æ’­æ”¾å“ªæ¡æ¶ˆæ¯ï¼ˆç”¨æ–‡æœ¬å½“ keyï¼›å¦‚æœä½ æœ‰ messageId æ›´å¥½ï¼‰
  const [playingKey, setPlayingKey] = useState(null);
  // æµè§ˆå™¨ audio å½“å‰æ˜¯å¦åœ¨æ’­ï¼ˆä¸ä¾èµ–é˜Ÿåˆ—é•¿åº¦ï¼‰
  const [isAudioPlaying, setIsAudioPlaying] = useState(false);

  // è‡ªåŠ¨æ’­æ”¾é—¨ç¦ï¼ˆè¢«æ‹¦æˆªæ—¶ç»™å‡ºâ€œç‚¹ä¸€ä¸‹å¯ç”¨éŸ³é¢‘â€çš„æŒ‰é’®ï¼‰
  const [needsUserTap, setNeedsUserTap] = useState(false);

  // ä¸ºâ€œæ¯æ¡æ°”æ³¡ç¼“å­˜å„è‡ªçš„éŸ³é¢‘åˆ†æ®µä¸åˆå¹¶ç»“æœâ€
  // Map<msgId, { items: Array<{ab:ArrayBuffer, mime:string}>, mergedUrl?: string }>
  const messageAudioStoreRef = useRef(new Map());
  const makeMsgId = () =>
    (window.crypto?.randomUUID?.() ?? `m_${Date.now().toString(36)}_${Math.random().toString(36).slice(2,8)}`);

  // ç”¨ä¸€ä¸ªç›‘å¬æ›¿æ¢ä½ ä¹‹å‰çš„ ended ç›‘å¬ï¼ˆé˜Ÿåˆ—ç»­æ’­ + çŠ¶æ€ç»´æŠ¤ï¼‰
  useEffect(() => {
    const a = audioRef.current; if (!a) return;

    const onPlay = () => setIsAudioPlaying(true);
    const onPause = () => setIsAudioPlaying(false);
    const onEnded = async () => {
      // æ’­å®Œå½“å‰æ®µï¼Œçœ‹çœ‹é˜Ÿåˆ—
      ttsQueueRef?.current?.shift?.();
      if (ttsQueueRef?.current?.length) {
        a.src = ttsQueueRef.current[0];
        await tryPlayElement(a);
      } else {
        setIsAudioPlaying(false);
        setPlayingKey(null);
        setStatus(s => (s.includes("Thinking") ? s : "Ready"));
      }
    };

    a.addEventListener("play", onPlay);
    a.addEventListener("pause", onPause);
    a.addEventListener("ended", onEnded);
    return () => {
      a.removeEventListener("play", onPlay);
      a.removeEventListener("pause", onPause);
      a.removeEventListener("ended", onEnded);
    };
  }, []);

  function stopSpeaking() {
    const a = audioRef.current; if (!a) return;
    try { a.pause(); a.currentTime = 0; } catch {}
    if (ttsQueueRef?.current) ttsQueueRef.current.length = 0; // æ¸…ç©ºé˜Ÿåˆ—ï¼ˆåˆ†æ®µæ¨¡å¼ï¼‰
    setPlayingKey(null);
    setIsAudioPlaying(false);
    setStatus(s => (s.includes("Thinking") ? s : "Ready"));
  }  

  async function tryPlayElement(a) {
    try {
      setNeedsUserTap(false);
      await a.play();
    } catch (e) {
      if (e?.name === "NotAllowedError") {
        // è¢«æµè§ˆå™¨è‡ªåŠ¨æ’­æ”¾ç­–ç•¥æ‹’ç»ï¼šæç¤ºç”¨æˆ·ç‚¹ä¸€ä¸‹
        setNeedsUserTap(true);
      }
    }
  }

  async function speakText(text, lang = "en") {
    if (!text || !audioRef.current) return;
    try {
      // å‘ˆç° Speakingâ€¦ çŠ¶æ€ï¼ˆä¸è¦†ç›– Thinkingâ€¦ï¼‰
      setStatus(s => (s.includes("Thinking") ? s : "Speakingâ€¦"));
  
      // å‘½ä¸­ç¼“å­˜ç›´æ¥æ’­
      const safe = sanitizeForTTS(text);
      let url = ttsCacheRef.current.get(safe);
      if (!url) {
        const resp = await fetch(`${API_BASE || ""}/api/tts`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text: safe, text_language: lang }),
        });
        if (!resp.ok) throw new Error(`TTS ${resp.status}`);
        const buf = await resp.arrayBuffer();
        const blob = new Blob([buf], { type: resp.headers.get("content-type") || "audio/mpeg" });
        url = URL.createObjectURL(blob);
        ttsCacheRef.current.set(safe, url);
      }
      
      try { audioRef.current.pause(); } catch {}
      try { audioRef.current.currentTime = 0; } catch {}
      audioRef.current.src = url;
      // æ’­æ”¾å¤±è´¥é™é»˜ï¼ˆä¾‹å¦‚ç”¨æˆ·æ²¡äº¤äº’å¯¼è‡´ autoplay é™åˆ¶ï¼‰
      await audioRef.current.play().catch(() => {});
    } catch (e) {
      setMessages(m => [...m, { role: "assistant", text: `TTS error: ${e?.message || e}` }]);
    } finally {
      setStatus(s => (s.includes("Thinking") ? s : "Ready"));
    }
  }
  
  // æ¸…ç†ç¼“å­˜ï¼ˆæ–°å¯¹è¯æ—¶ç”¨ï¼‰
  function clearTtsCache() {
    for (const u of ttsCacheRef.current.values()) URL.revokeObjectURL(u);
    ttsCacheRef.current.clear();
  }

  // ç¼“å­˜ï¼šchunk æ–‡æœ¬ -> { url, blob, ab(ArrayBuffer), mime }
  const ttsObjCacheRef = useRef(new Map());
  // æ’­æ”¾é˜Ÿåˆ—ï¼šåªå­˜ URL
  const ttsQueueRef = useRef([]);
  // å½“å‰ç­”æ¡ˆçš„åˆ†æ®µåŸå§‹éŸ³é¢‘ï¼ˆç”¨äºâ€œä¸€æ¬¡æ€§é‡æ’­â€åˆå¹¶ï¼‰
  const currentAnswerAudioRef = useRef({ key: "", items: [], mergedUrl: null });

  function resetCurrentAnswerAudio(key) {
    // é‡Šæ”¾æ—§çš„åˆå¹¶ URL
    const cur = currentAnswerAudioRef.current;
    if (cur.mergedUrl) { URL.revokeObjectURL(cur.mergedUrl); }
    currentAnswerAudioRef.current = { key, items: [], mergedUrl: null };
  }

  const ttsInflightRef = useRef(new Map());

  // å–/ç”Ÿæˆä¸€ä¸ª chunk çš„éŸ³é¢‘
  async function getTtsAudioObj(chunkText, lang="en") {
    const safe = sanitizeForTTS(chunkText);
    const cacheKey = `${lang}::${safe}`;
    if (ttsObjCacheRef.current.has(cacheKey)) return ttsObjCacheRef.current.get(cacheKey);

    // å¦‚æœå·²æœ‰åŒ key çš„è¯·æ±‚åœ¨é£ï¼Œç›´æ¥å¤ç”¨åŒä¸€ä¸ª Promise
    if (ttsInflightRef.current.has(cacheKey)) return ttsInflightRef.current.get(cacheKey);

    const p = (async () => {
    const resp = await fetch(`${API_BASE || ""}/api/tts`, {
        method: "POST",
        headers: { "Content-Type": "application/json", "Accept": "audio/mpeg" },
        body: JSON.stringify({ text: safe, text_language: lang }),
      });
      if (!resp.ok) throw new Error(`TTS ${resp.status}`);
      const ab = await resp.arrayBuffer();
      const mime = resp.headers.get("content-type") || "audio/mpeg";
      const blob = new Blob([ab], { type: mime });
      const url = URL.createObjectURL(blob);
      const obj = { url, blob, ab, mime };
      ttsObjCacheRef.current.set(cacheKey, obj);
      return obj;
    })();
    ttsInflightRef.current.set(cacheKey, p);
    try { return await p; }
    finally { ttsInflightRef.current.delete(cacheKey); }
  }

  // å…¥é˜Ÿå¹¶åœ¨ç©ºé—²æ—¶å¯åŠ¨æ’­æ”¾
  async function enqueueAndPlay(audioUrl) {
    ttsQueueRef.current.push(audioUrl);
    const a = audioRef.current;
    if (!a) return;
    // å¦‚æœå½“å‰ä¸åœ¨æ’­ï¼Œç«‹åˆ»æ’­é˜Ÿé¦–
    if (a.paused && ttsQueueRef.current.length === 1) {
      a.src = ttsQueueRef.current[0];
      await tryPlayElement(a);
    }
  }


  // ä¸»æ¥å£ï¼šé•¿æ–‡æœ¬æ™ºèƒ½åˆ†æ®µ â†’ åˆ°æ®µå³æ’­ï¼›replay=true æ—¶åˆå¹¶ä¸ºå• WAV ä¸€æ¬¡æ€§æ’­æ”¾
  async function speakTextSmart(fullText, lang = "en", { replay = false, msgId } = {}) {
    const sentences = splitIntoSentencesSmart(fullText);
    const chunks = groupSentencesToChunks(sentences, { maxChars: 400, minSent: 2, maxSent: 3 });

    if (!replay) {
      setStatus(s => (s.includes("Thinking") ? s : "Speakingâ€¦"));
      // ä¸ºè¯¥æ¶ˆæ¯å‡†å¤‡ç¼“å­˜å®¹å™¨
      if (msgId && !messageAudioStoreRef.current.has(msgId)) {
        messageAudioStoreRef.current.set(msgId, { items: [], mergedUrl: null });
      }
      // é¡ºåºç”Ÿæˆå¹¶å…¥é˜Ÿï¼›ç¬¬ä¸€æ®µåˆ°å°±å…ˆæ’­
      for (let i = 0; i < chunks.length; i++) {
        const obj = await getTtsAudioObj(chunks[i], lang);
        // å­˜åˆ°â€œå½“å‰ç­”æ¡ˆâ€çš„åŸå§‹åˆ†æ®µï¼ˆç”¨äºåˆå¹¶ï¼‰
        if (msgId) {
          const entry = messageAudioStoreRef.current.get(msgId);
          entry.items.push({ ab: obj.ab, mime: obj.mime });
        }
        await enqueueAndPlay(obj.url);
      }
      setStatus(s => (s.includes("Thinking") ? s : "Ready"));
      return;
    }

    // â€”â€” é‡æ’­ï¼šæŒ‰è¯¥æ¶ˆæ¯ id çš„åˆ†æ®µåˆå¹¶ä¸ºå•ä¸ª WAV å†æ’­ â€”â€”
    try {
      setStatus("Preparingâ€¦");
      const entry = msgId ? messageAudioStoreRef.current.get(msgId) : null;
      const items = entry?.items ?? [];
      if (!items.length) {
        // æ²¡ç¼“å­˜ï¼ˆä¾‹å¦‚åˆ·æ–°åï¼‰ï¼Œé€€åŒ–ä¸ºé¡ºåºé‡æ’­å¹¶é‡æ–°ç¼“å­˜
        return speakTextSmart(fullText, lang, { replay: false, msgId });
      }
      if (entry.mergedUrl) {
        // å·²ç»åˆå¹¶è¿‡ï¼Œç›´æ¥æ’­
        ttsQueueRef.current.length = 0;
        const a = audioRef.current;
        if (a) {
          a.pause(); a.currentTime = 0; a.src = entry.mergedUrl;
          await tryPlayElement(a);
        }
        setStatus("Ready");
        return;
      }
      const ctx = new (window.AudioContext || window.webkitAudioContext)();
      try { await ctx.resume?.(); } catch {}
      const decoded = [];
      for (const it of items) {
        const buf = await ctx.decodeAudioData(it.ab.slice(0)); // decode mp3/wav â†’ AudioBuffer
        decoded.push(buf);
      }
      const sampleRate = decoded[0].sampleRate;
      const channels = decoded[0].numberOfChannels;
      // ç®€å•å‡è®¾ sampleRate/é€šé“ä¸€è‡´ï¼ˆå¤§å¤šæ•° TTS ä¸€è‡´ï¼‰ï¼›å¦åˆ™å¯åšé‡é‡‡æ ·
      let totalLen = 0;
      for (const b of decoded) totalLen += b.length;

      // æ‹¼æ¥åˆ°ä¸€ä¸ªå¤§çš„ AudioBuffer
      const out = ctx.createBuffer(channels, totalLen, sampleRate);
      let offset = 0;
      for (const b of decoded) {
        for (let ch = 0; ch < channels; ch++) {
          out.getChannelData(ch).set(b.getChannelData(ch), offset);
        }
        offset += b.length;
      }

      // å¯¼å‡ºä¸º WAV
      const mergedWavBlob = audioBufferToWavBlob(out);
      const mergedUrl = URL.createObjectURL(mergedWavBlob);
      // è®°å½•ï¼Œé¿å…ä¸‹æ¬¡å†åš
      if (entry.mergedUrl) URL.revokeObjectURL(entry.mergedUrl);
      entry.mergedUrl = mergedUrl;

      // æ¸…ç©ºé˜Ÿåˆ—å¹¶ä¸€æ¬¡æ€§æ’­æ”¾
      ttsQueueRef.current.length = 0;
      const a = audioRef.current;
      if (a) {
        a.pause();
        a.currentTime = 0;
        a.src = mergedUrl;
        await tryPlayElement(a);
      }
    } catch (e) {
      setMessages(m => [...m, { role: "assistant", text: `Replay merge error: ${e?.message || e}` }]);
    } finally {
      setStatus("Ready");
    }
  }

  // æŠŠ AudioBuffer å¯¼æˆ WAVï¼ˆfloat32 â†’ 16-bit PCMï¼‰
  function audioBufferToWavBlob(buffer) {
    const numCh = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const numFrames = buffer.length;
    const bytesPerSample = 2; // 16-bit
    const blockAlign = numCh * bytesPerSample;
    const dataSize = numFrames * blockAlign;
    const headerSize = 44;
    const ab = new ArrayBuffer(headerSize + dataSize);
    const dv = new DataView(ab);
    let p = 0;

    // RIFF header
    writeStr("RIFF"); u32(headerSize + dataSize - 8);
    writeStr("WAVE");
    writeStr("fmt "); u32(16); u16(1); u16(numCh); u32(sampleRate); u32(sampleRate * blockAlign); u16(blockAlign); u16(16);
    writeStr("data"); u32(dataSize);

    // Interleave
    const chData = [];
    for (let ch = 0; ch < numCh; ch++) chData.push(buffer.getChannelData(ch));
    for (let i = 0; i < numFrames; i++) {
      for (let ch = 0; ch < numCh; ch++) {
        let s = Math.max(-1, Math.min(1, chData[ch][i]));
        s = s < 0 ? s * 0x8000 : s * 0x7FFF;
        dv.setInt16(p, s, true); p += 2;
      }
    }
    return new Blob([ab], { type: "audio/wav" });

    // helpers
    function writeStr(s){ for (let i=0;i<s.length;i++) dv.setUint8(p++, s.charCodeAt(i)); }
    function u16(v){ dv.setUint16(p, v, true); p+=2; }
    function u32(v){ dv.setUint32(p, v, true); p+=4; }
  }

  useEffect(() => {
    if (!START_FRESH_ON_LOAD) return;
    try {
      localStorage.removeItem("dify_conversation_id");
      localStorage.removeItem("dify_conversation_ts"); // if you ever added TTL
    } catch {}
    setConversationId("");   // force a new thread
    setMessages([]);         // clear UI
    setStatus("Ready");
  }, []); // runs once after first render

  useEffect(() => {
    if (scrollerRef.current) {
      scrollerRef.current.scrollTo({ top: scrollerRef.current.scrollHeight, behavior: "smooth" });
    }
  }, [messages]);

  // Clean up on unload (stop mic if active)
  useEffect(() => {
    return () => {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
        mediaRecorderRef.current.stop();
      }
      streamRef.current?.getTracks()?.forEach(t => t.stop());
      wsRef.current?.close?.();
    };
  }, []);


  async function handleVoiceClip() {
    try {
      const parts = chunksRef.current;
      chunksRef.current = [];
      if (!parts.length) { setStatus("Ready"); return; }
  
      // ç”Ÿæˆ Blobï¼ˆæµè§ˆå™¨å®é™…å¯èƒ½æ˜¯ webm/opusï¼›åç«¯èƒ½åƒå°±è¡Œï¼‰
      const blob = new Blob(parts, { type: parts[0]?.type || "audio/webm" });
  
      setStatus("Uploadingâ€¦");
      const fileId = await asrUpload(blob);     // â‘  ä¸Šä¼ ï¼Œæ‹¿ id
      setStatus("Transcribingâ€¦");
      const text = await asrRun(fileId);        // â‘¡ è°ƒ ASRï¼Œæ‹¿æ–‡æœ¬
  
      // æŠŠè¯†åˆ«æ–‡æœ¬å½“ä½œç”¨æˆ·å‘è¨€æ’å…¥ï¼Œç„¶åå¤ç”¨ä½ ç°æœ‰çš„ runChat
      if (text && text.trim()) {
        setMessages(m => [...m, { role: "user", text }]);
        setStatus("Thinkingâ€¦");
        await runChat(text);
      } else {
        setMessages(m => [...m, { role: "assistant", text: "ASR returned empty text." }]);
        setStatus("Ready");
      }
    } catch (e) {
      setMessages(m => [...m, { role: "assistant", text: `ASR error: ${e?.message || e}` }]);
      setStatus("Ready");
    }
  }

  async function asrUpload(blob) {
    // POST /api/asr/upload  -> é€ä¼ åˆ° https://agent.helport.ai/v1/files/upload
    const form = new FormData();
    form.append("user", "abc123"); // ä½ ç¤ºä¾‹é‡Œçš„å­—æ®µ
    form.append("file", new File([blob], "recording.webm", { type: blob.type || "audio/webm" }));
  
    const resp = await fetch(`${API_BASE || ""}/api/asr/upload`, { method: "POST", body: form });
    if (!resp.ok) throw new Error(`Upload failed: ${resp.status}`);
    const json = await resp.json();
    const fileId = json?.id;
    if (!fileId) throw new Error("Upload OK but no file id.");
    return fileId;
  }
  
  async function asrRun(fileId) {
    // POST /api/asr/run -> é€ä¼ åˆ° https://agent.helport.ai/v1/workflows/run
    const payload = {
      inputs: { speech: { transfer_method: "local_file", upload_file_id: fileId, type: "audio" } },
      response_mode: "blocking",
      user: ASR_USER,
    };
    const resp = await fetch(`${API_BASE || ""}/api/asr/run`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(payload),
    });
    if (!resp.ok) throw new Error(`ASR failed: ${resp.status}`);
    const json = await resp.json();
    return json?.data?.outputs?.text || "";
  }
  
  function sanitizeForTTS(input) {
    if (!input) return "";
    let text = input;
  
    // 1) åˆ—è¡¨é¡¹ï¼š" - " -> "â€¢ "ï¼ˆä»…è¡Œé¦–çš„çŸ­æ¨ªï¼‰
    text = text.replace(/^\s*-\s+/gm, "â€¢ ");
  
    // 2) æ•°å€¼åŒºé—´: "3-5" æˆ– "0.7-0.8" -> "3 to 5"
    text = text.replace(
      /(\d+(?:\.\d+)?)\s*-\s*(\d+(?:\.\d+)?)(?=\s*[%a-zA-Z]|[^\d]|$)/g,
      "$1 to $2"
    );
  
    // 3) è¯å†…è¿å­—ç¬¦: "cash-out"ã€"follow-up" -> "cash out"ã€"follow up"
    // ä»…æ›¿æ¢ ASCII '-'ï¼Œä¸åŠ¨ U+2011/2010 ç­‰çœŸæ­£çš„è¿å­—ç¬¦
    text = text.replace(/(\p{L})-(?=\p{L})/gu, "$1 ");
  
    // 4) è´Ÿæ•°ï¼ˆæ™®é€šåœºæ™¯ï¼‰: "-5" -> "negative 5"
    text = text.replace(/(^|[^\d])-(\d+(?:\.\d+)?)(?![\d-])/g, "$1negative $2");
  
    // 4.1) è´§å¸è´Ÿæ•°: "$-100" -> "negative $100"
    text = text.replace(/([$â‚¬Â£])-(\d+(?:[\d,\.])*)/g, "negative $1$2");
  
    // 4.2) è´Ÿç™¾åˆ†æ¯”: "-5%" -> "negative 5 percent"
    text = text.replace(/-(\d+(?:\.\d+)?)\s*%/g, "negative $1 percent");
  
    // 5) en/em dash è¯»æˆåœé¡¿
    text = text.replace(/[â€“â€”]/g, ", ");
  
    // 6) åˆå¹¶ç©ºæ ¼
    text = text.replace(/\s{2,}/g, " ").trim();
  
    return text;
  }  
  

  // ----- Start mic + (optionally) WS -----
  const startConversation = async () => {
    try {
      // Mic first (so UI toggles even if WS fails)
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;

      const mime = pickAudioMime();
      const opts = mime ? { mimeType: mime } : undefined;
      const mr = new MediaRecorder(stream, opts);
      mediaRecorderRef.current = mr;

      setRecording(true);
      setStatus("Listeningâ€¦");
      chunksRef.current = [];

      mr.ondataavailable = async (e) => {
        if (e.data && e.data.size) chunksRef.current.push(e.data);
      };
      mr.onstop = () => { void handleVoiceClip(); };

      mr.start(220); // low-latency chunks
    } catch (err) {
      console.error(err);
      setStatus(`Mic error: ${err?.name || err?.message || err}`);
      setRecording(false);
      // Be sure to stop any tracks if partially opened
      streamRef.current?.getTracks()?.forEach(t => t.stop());
      setMode("type"); // switch to type mode on mic fail
    }
  };

  // ----- Send typed message -----
  const sendTextMessage = async () => {
    if (!textInput?.trim()) return;
    // Optimistically append user message
    setMessages(m => {
      // Append user message, then a provisional assistant 'thinking' bubble
      return [...m, { role: "user", text: textInput }, { role: "assistant", text: "Thinkingâ€¦", provisional: true }];
    });
    setStatus("Thinkingâ€¦");

    await runChat(textInput);

    setTextInput("");
  };

  // ----- Call Dify Chatflow via server proxy (blocking) -----
  const runChat = async (query) => {
    setStatus("Chatflowâ€¦");
    try {
      const body = {
        query: query,
        inputs: {
          // carry forward any app vars you used in workflow, e.g. datasets, toggles, etc.
          qa_dataset_id: "a034b9b4-9b64-40d2-b3c1-951281f84dc6",
        },
        conversation_id: conversationId || undefined,
        user: "Enoch@HELPORT.AI",
        response_mode: "blocking",
      };

      const resp = await fetch(`${API_BASE}/api/chat`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(body),
      });
      const json = await resp.json();

      // Dify Chatflow commonly returns: { answer, conversation_id, ... }
      const answer =
        (typeof json?.answer === "string" && json.answer) ||
        (typeof json?.data?.answer === "string" && json.data.answer) ||
        // fallback if a tool returns a structured output
        JSON.stringify(json, null, 2);

      if (json?.conversation_id && json.conversation_id !== conversationId) {
        setConversationId(json.conversation_id);
        try { localStorage.setItem("dify_conversation_id", json.conversation_id); } catch {}
      }

      const finalText = String(answer);
      let targetIndex = -1;
      setMessages((m) => {
        const copy = m.slice();
        for (let i = copy.length - 1; i >= 0; i--) {
          if (copy[i].role === "assistant" && copy[i].provisional) {
            const id = makeMsgId();
            copy[i] = { role: "assistant", text: finalText, id };
            // ç«‹åˆ»å¼€å§‹å¯¹è¯¥æ¶ˆæ¯åšåˆ†æ®µ TTSï¼ˆåˆ°æ®µå³æ’­ï¼‰ï¼Œå¹¶æŠŠåˆ†æ®µç¼“å­˜åˆ°è¯¥ id ä¸‹
            setPlayingKey(id);
            setTimeout(() => speakTextSmart(finalText, "en", { replay: false, msgId: id }), 0);
            targetIndex = i;
            break;
          }
        }
        if (targetIndex === -1) {
          const id = makeMsgId();
          copy.push({ role: "assistant", text: finalText, id });
          setPlayingKey(id);
          setTimeout(() => speakTextSmart(finalText, "en", { replay: false, msgId: id }), 0);
          targetIndex = copy.length - 1;
        }
        return copy;
      });
      setStatus("Ready");
    } catch (err) {
      setMessages((m) => {
        const copy = m.slice();
        for (let i = copy.length - 1; i >= 0; i--) {
          if (copy[i].role === "assistant" && copy[i].provisional) {
            copy[i] = { role: "assistant", text: `Chatflow error: ${err?.message || err}` };
            return copy;
          }
        }
        return [...m, { role: "assistant", text: `Chatflow error: ${err?.message || err}` }];
      });
      setStatus(`Chatflow error: ${err?.message || err}`);
    }
  };


  // ----- End mic -----
  const endConversation = () => {
    try {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
        mediaRecorderRef.current.stop();
      }
    } catch {}
    // Stop all tracks to release mic light in the browser UI
    streamRef.current?.getTracks()?.forEach(t => t.stop());
    streamRef.current = null;
    setRecording(false);
  };

  // put inside your App component
  const newConversation = () => {
    // Stop mic cleanly if recording
    try {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
        mediaRecorderRef.current.stop();
      }
    } catch {}
    streamRef.current?.getTracks()?.forEach(t => t.stop());
    streamRef.current = null;
    setRecording(false);

    // Optionally close the WS session (not required for Dify new chat, but tidy)
    try { wsRef.current?.close?.(); } catch {}

    // Clear persisted conversation id (and any timestamp you might add later)
    try {
      localStorage.removeItem("dify_conversation_id");
      localStorage.removeItem("dify_conversation_ts"); // if you adopt TTL later
    } catch {}

    // Reset UI state
    stopSpeaking();
    clearTtsCache();
    // é‡Šæ”¾æ¯æ¡æ¶ˆæ¯çš„åˆå¹¶ URLï¼Œé¿å…å†…å­˜æ³„éœ²
    for (const entry of messageAudioStoreRef.current.values()) {
      if (entry?.mergedUrl) URL.revokeObjectURL(entry.mergedUrl);
    }
    messageAudioStoreRef.current.clear();
    setConversationId("");
    setMessages([]);
    setStatus("Ready");
  };

  // ===== Derived gating for CTA =====
  const hasProvisionalAssistant = messages.some(
    (m) => m.role === "assistant" && m.provisional
  );

  // â€œæ–‡å­—è¿˜æ²¡ Readyï¼Ÿâ€
  const isThinking =
    hasProvisionalAssistant ||
    /Thinking|Chatflow|Uploading|Transcribing/i.test(status);

  // â€œéŸ³é¢‘è¿˜åœ¨è¯´ï¼Ÿâ€
  const isSpeaking =
    isAudioPlaying ||
    needsUserTap ||
    /Speaking|Preparing/i.test(status);

  // ç»Ÿä¸€ç”Ÿæˆ Voice CTA çš„æ¸²æŸ“æ•°æ®
  const voiceCta = (() => {
    if (recording) {
      return {
        label: "End conversation",
        onClick: endConversation,
        disabled: false,
        danger: true,
        icon: <Square size={16} />,
        aria: "End conversation",
      };
    }
    if (isThinking) {
      return {
        label: "Thinking",
        onClick: undefined,
        disabled: true,
        danger: false,
        icon: <Loader2 size={16} className="animate-spin" />,
        aria: "Thinkingâ€¦ please wait",
      };
    }
    if (isSpeaking) {
      return {
        label: "Speaking",
        onClick: undefined,
        disabled: true,
        danger: false,
        icon: <Volume2 size={16} />,
        aria: "Speakingâ€¦ please wait",
      };
    }
    return {
      label: "Start conversation",
      onClick: startConversation,
      disabled: false,
      danger: false,
      icon: <Mic size={16} />,
      aria: "Start conversation",
    };
  })();

  // ====== NEW: minimal helpers to meet your two UI requirements ======
  // Derived status: hide "Ready/Listening" when in Type mode; only show when thinking
  const displayStatus =
    mode === "type" ? (status.includes("Thinking") ? status : "") : status;

  const statusIcon = () => {
    if (!displayStatus) return null;
    if (displayStatus.includes("Listening")) return <Mic size={14} />;
    if (displayStatus.includes("Thinking") || displayStatus.includes("Chatflow"))
      return <Loader2 size={14} className="animate-spin" />;
    return <Volume2 size={14} />;
  };

  // Mode switches: when entering Type, stop mic and clear idle status
  const switchToVoice = () => setMode("voice");
  const switchToType = () => {
    if (recording) endConversation();
    setMode("type");
    if (!status.includes("Thinking")) setStatus("");
  };

  // Mode-aware empty-state copy
  const EmptyHint = () => (
    <div style={styles.emptyText}>
      {mode === "voice" ? (
        <>
          Click <span style={{ fontWeight: 600, color: ACCENT }}>Start conversation</span> and speak â€” weâ€™ll transcribe, retrieve, and reply.
        </>
      ) : (
        <>
          <div>
            Type a question below and press <span style={{ fontWeight: 600, color: ACCENT }}>Enter</span> â€” weâ€™ll retrieve and reply.
          </div>
        </>
      )}
    </div>
  );
  // ================================================================

  // ----- Styles -----
  const styles = {
    page: {
      minHeight: "100vh",
      backgroundImage: BG_GRADIENT,
      backgroundColor: "#ffffff",
      color: "#0a0a0a",
      display: "flex",
      flexDirection: "column",
    },
    topbar: {
      maxWidth: 1100,
      width: "100%",
      margin: "0 auto",
      padding: "24px 20px",
      display: "flex",
      alignItems: "center",
      gap: 12,
    },
    toggleDivider: { width: 1, height: 18, background: "rgba(0,0,0,0.08)", margin: "0 2px" },
    brand: { height: 36 },
    spacer: { marginLeft: "auto", fontSize: 12, opacity: 0.6 },
    hero: { maxWidth: 820, width: "100%", margin: "0 auto 16px", padding: "0 20px", textAlign: "center" },
    h1: { fontSize: 36, fontWeight: 600, letterSpacing: -0.4, margin: 0 },
    p: { marginTop: 8, opacity: 0.7 },
    cardWrap: { maxWidth: 820, width: "100%", margin: "0 auto", padding: "0 20px" },
    card: {
      borderRadius: 24,
      border: "2px solid rgba(0,0,0,0.08)",
      background: "rgba(255,255,255,0.85)",
      backdropFilter: "blur(8px)",
      boxShadow: "0 6px 24px rgba(0,0,0,0.06)",
      overflow: "hidden",
    },
    scroll: { height: "56vh", overflowY: "auto", padding: 20 },
    emptyText: { opacity: 0.6, fontSize: 18, textAlign: 'center' },
    row: (justify) => ({ display: "flex", justifyContent: justify, marginBottom: 10 }),
    bubble: (me, provisional) => ({
      background: me ? ACCENT : "rgba(255,255,255,0.95)",
      color: me ? "white" : "#0a0a0a",
      borderRadius: 18,
      padding: "10px 14px",
      maxWidth: "85%",
      boxShadow: "0 1px 6px rgba(0,0,0,0.06)",
      whiteSpace: "pre-wrap",
      opacity: provisional ? 0.7 : 1,          // <â€” dim while provisional
      fontStyle: provisional ? "italic" : "normal",
    }),
    controls: {
      display: "flex",
      alignItems: "center",
      justifyContent: "space-between", // push apart
      padding: 16,
      borderTop: "1px solid rgba(0,0,0,0.08)",
    },
    cta: (danger) => ({
      display: "inline-flex",
      alignItems: "center",
      gap: 8,
      border: "none",
      borderRadius: 30,
      padding: "12px 18px",
      fontSize: 14,
      fontWeight: 600,
      color: "#fff",
      background: danger ? "#ff453a" : ACCENT,
      cursor: "pointer",
      boxShadow: "0 4px 12px rgba(0,0,0,0.12)",
    }),
    ctaDisabled: {
      opacity: 0.5,
      cursor: "not-allowed",
      filter: "grayscale(15%)",
    },    
    status: { fontSize: 12, opacity: 0.7, display: "inline-flex", alignItems: "center", gap: 6 },
    footer: { maxWidth: 1100, width: "100%", margin: "0 auto", padding: "32px 20px", textAlign: "center", fontSize: 12, opacity: 0.6 },

    avatar: {
      width: 50,
      height: 50,
      borderRadius: "50%",
      flex: "0 0 36px",
      objectFit: "cover",
      boxShadow: "0 2px 6px rgba(0,0,0,0.12)",
      marginRight: 10,              // space between avatar and bubble
    },
    rowWithAvatar: {
      display: "flex",
      alignItems: "flex-start",
      gap: 0,
      marginBottom: 10,
    },

    input: {
      padding: "14px 16px",
      borderRadius: 20,
      minWidth: 300,
      outline: "none",                     
      transition: "border-color .15s, box-shadow .15s",
      fontWeight: 400,
      color: "#0a0a0a",
      boxShadow: "0 4px 12px rgba(0,0,0,0.12)",
      border: `1px solid ${ACCENT}`
    },
    inputFocused: {
      border: `1px solid ${ACCENT}`,       // green border
      boxShadow: `0 0 0 4px rgba(0,195,137,0.15) inset`, // soft glow
    },

    // in styles
    toggle: {
      display: "flex",
      alignItems: "center",
      border: `1px solid ${ACCENT}`,
      borderRadius: 30,
      overflow: "hidden",
      background: "#ffffff",
      position: "relative",
      boxShadow: "0 4px 12px rgba(0,0,0,0.12)",
    },
    toggleBtn: {
      flex: 1,
      minWidth: 0,
      padding: "12px 18px",      
      fontSize: 14,
      fontWeight: 600,
      border: "none",
      background: "transparent",
      color: ACCENT,
      cursor: "pointer",
      lineHeight: 1,
      textAlign: "center",
      outline: "none",
      boxShadow: "none",
    },
    toggleBtnActive: {
      background: ACCENT,
      color: "#ffffff",
    },

    toggleBtnLeftPad:  { padding: "12px 22px 12px 18px" },  // a touch more right pad
    toggleBtnRightPad: { padding: "12px 18px 12px 22px" },  // a touch more left pad

  };

  return (
    <div style={styles.page}>
      {/* Top Bar */}
      <div style={styles.topbar}>
        {/* Put your logo at public/helport.png */}
        <img src="/helport.png" alt="Helport AI" style={styles.brand} />
        <div style={styles.spacer}>Session: {sessionId || "new"} Â· {connected ? "Online" : "Offline"}</div>
      </div>

      {/* Hero */}
      <section style={styles.hero}>
        <motion.h1 initial={{opacity:0,y:6}} animate={{opacity:1,y:0}} style={styles.h1}>Toby Clone Bot</motion.h1>
        <motion.p initial={{opacity:0,y:6}} animate={{opacity:1,y:0}} style={styles.p}>
          Voice demo of our sales knowledge base â€” ask anything and hear the answer in Toby&apos;s voice.
        </motion.p>
      </section>

      {/* Card */}
      <main style={styles.cardWrap}>
        <div style={styles.card}>
          {/* Chat Scroll */}
          <div ref={scrollerRef} style={styles.scroll}>
            {messages.length === 0 ? (
              <div style={{ height: "100%", display: "grid", placeItems: "center" }}>
                <EmptyHint />
              </div>
            ) : (
              <div>
                {messages.map((m, i) => (
                  <motion.div key={m.id ?? i}
                    initial={{ opacity: 0, y: 4 }}
                    animate={{ opacity: 1, y: 0 }}
                    style={m.role === "assistant" ? styles.rowWithAvatar : styles.row("flex-end")}
                  >
                    {m.role === "assistant" && <img src={AVATAR_URL} alt="Agent" style={styles.avatar} />}

                    <div style={{ display: "flex", alignItems: "center", gap: 6 }}>
                    <div style={styles.bubble(m.role === "user", m.provisional)}>
                      {m.text}
                    </div>
                    {m.role === "assistant" && !m.provisional && (
                      (playingKey === m.id && isAudioPlaying) ? (
                        <button
                          onClick={stopSpeaking}
                          title="Stop"
                          aria-label="Stop audio"
                          style={{
                            border: "none",
                            background: "transparent",
                            cursor: "pointer",
                            display: "inline-flex",
                            alignItems: "center",
                            padding: 4,
                            opacity: 0.9,
                          }}
                        >
                          <Square size={16} />
                        </button>
                      ) : (
                        <button
                          onClick={() => {
                            stopSpeaking();
                            setPlayingKey(m.id);
                            speakTextSmart(m.text, "en", { replay: true, msgId: m.id });
                          }}
                          title="Replay"
                          aria-label="Replay audio"
                          style={{
                            border: "none",
                            background: "transparent",
                            cursor: "pointer",
                            display: "inline-flex",
                            alignItems: "center",
                            padding: 4,
                            opacity: 0.7,
                          }}
                        >
                          <Volume2 size={16} />
                        </button>
                      )
                    )}

                  </div>
                  </motion.div>
                ))}
              </div>
            )}
          </div>

          {/* Controls */}
          <div style={styles.controls}>
            <div style={{ display: "flex", alignItems: "center", gap: 12 }}>
              <div style={styles.toggle} role="tablist" aria-label="Input mode">
                <button
                  role="tab"
                  aria-selected={mode === "voice"}
                  onClick={switchToVoice}
                  style={{ ...styles.toggleBtn, ...(mode === "voice" ? styles.toggleBtnActive : null) }}
                >
                  Voice
                </button>
                <div aria-hidden style={styles.toggleDivider} />
                <button
                  role="tab"
                  aria-selected={mode === "type"}
                  onClick={switchToType}
                  style={{ ...styles.toggleBtn, ...(mode === "type" ? styles.toggleBtnActive : null), ...styles.toggleBtnRightPad, ...styles.toggleBtnLeftPad }}
                >
                  Text
                </button>
              </div>
              {displayStatus && (
                <span style={styles.status}>
                  {statusIcon()}
                  {displayStatus}
                </span>
              )}
              {needsUserTap && (
                <button
                  onClick={() => { try { audioRef.current?.play(); } catch {} }}
                  style={{ marginLeft: 8, border: '1px solid ' + ACCENT, color: ACCENT, background: '#fff', borderRadius: 20, padding: '6px 10px', fontSize: 12, cursor: 'pointer' }}
                  title="Enable audio"
                >
                ğŸ”ˆ ç‚¹å‡»å¯ç”¨éŸ³é¢‘
                </button>
              )}
            </div>

            {/* Controls: either show recording CTA or text input depending on mode */}
              {mode === "voice" ? (
                <button
                  onClick={voiceCta.onClick}
                  disabled={voiceCta.disabled}
                  style={{
                    ...styles.cta(voiceCta.danger),
                    ...(voiceCta.disabled ? styles.ctaDisabled : null),
                  }}
                  aria-label={voiceCta.aria}
                >
                  {voiceCta.icon} {voiceCta.label}
                </button>
              ) : (
              <div style={{ display: "flex", gap: 8, alignItems: "center" }}>
                <input
                  value={textInput}
                  onChange={(e) => setTextInput(e.target.value)}
                  onKeyDown={(e) => { if (e.key === "Enter") sendTextMessage(); }}
                  onFocus={() => setInputFocused(true)}
                  onBlur={() => setInputFocused(false)}
                  placeholder="Type your question and press Enter"
                  style={{ ...styles.input, ...(inputFocused ? styles.inputFocused : null) }}
                />

                <button onClick={sendTextMessage} style={styles.cta(false)} aria-label="Send message">
                  <ArrowUp size={16} />
                </button>
              </div>
            )}
            <button
              onClick={newConversation}
              style={{ ...styles.cta(false), background: '#ffffff', color: ACCENT, border: `1px solid ${ACCENT}` }}
              aria-label="Start a new conversation"
            >
              New conversation
            </button>
          </div>
        </div>
      </main>

      <audio ref={audioRef} preload="auto" playsInline/>

      {/* Footer */}
      <footer style={styles.footer}>
        Â© {new Date().getFullYear()} Helport AI Â· Built for live demos Â· Voice: top-sales-voice-001
      </footer>
    </div>
  );
}